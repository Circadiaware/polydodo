{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep stages classification pipeline\n",
    "___\n",
    "\n",
    "This notebooks aims to construct our feature matrix from our sleep records [sleep-edf](https://physionet.org/content/sleep-edfx/1.0.0/) from _physionet_. \n",
    "\n",
    "We will reuse the chosen features from our exploration notebook.\n",
    "\n",
    "Since all of the dataset cannot be loaded in memory at the same time, we will have to implement a pipeline, where each step can then be run with only one recording at a time. At the end of this notebook, we will be able to concatenate all resulting features in a single matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import (train_test_split, KFold)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, f1_score)\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import psd_welch\n",
    "from scipy.stats import (skew, kurtosis)\n",
    "from scipy.signal import butter\n",
    "\n",
    "from utils import fetch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a few constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS_AWAKE_MORNING = 60\n",
    "    \n",
    "SAMPLING_FREQ = 100\n",
    "NYQUIST_FREQ = SAMPLING_FREQ/2\n",
    "EPOCH_DURATION = 30. # in seconds\n",
    "MAX_TIME = EPOCH_DURATION - 1. / SAMPLING_FREQ  # tmax in included\n",
    "\n",
    "EEG_CHANNELS = [\n",
    "    'EEG Fpz-Cz',\n",
    "    'EEG Pz-Oz'\n",
    "]\n",
    "\n",
    "# Sleep stages values\n",
    "W = 0\n",
    "N1 = 1\n",
    "N2 = 2\n",
    "N3 = 3\n",
    "REM = 4\n",
    "\n",
    "# EEG sub bands labels\n",
    "DELTA = \"delta\"\n",
    "THETA = \"theta\"\n",
    "ALPHA = \"alpha\"\n",
    "SIGMA = \"sigma\"\n",
    "BETA = \"beta\"\n",
    "\n",
    "ANNOTATIONS_EVENT_ID = {\n",
    "    'Sleep stage W': W,\n",
    "    'Sleep stage 1': N1,\n",
    "    'Sleep stage 2': N2,\n",
    "    'Sleep stage 3': N3,\n",
    "    'Sleep stage 4': N3,\n",
    "    'Sleep stage R': REM\n",
    "}\n",
    "\n",
    "EVENT_ID = {\n",
    "    \"W\": W,\n",
    "    \"N1\": N1,\n",
    "    \"N2\": N2,\n",
    "    \"N3\": N3,\n",
    "    \"REM\": REM\n",
    "}\n",
    "\n",
    "FREQ_BANDS_RANGE = {\n",
    "    DELTA: [0.5, 4.5],\n",
    "    THETA: [4.5, 8.5],\n",
    "    ALPHA: [8.5, 11.5],\n",
    "    SIGMA: [11.5, 15.5],\n",
    "    BETA: [15.5, 30]\n",
    "}\n",
    "\n",
    "FREQ_BANDS_ORDERS = {\n",
    "    DELTA: 5,\n",
    "    THETA: 8,\n",
    "    ALPHA: 9,\n",
    "    SIGMA: 9,\n",
    "    BETA: 14\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "___\n",
    "\n",
    "Our dataset consists of 39 recordings, each containing about 20 hours of EEG, EOG, EMG, and other signals, sampled at 100 or 1Hz.\n",
    "\n",
    "### 1. Loading recordings informations\n",
    "___\n",
    "\n",
    "This file contains information about when a recording was started, at which time the subject went to bed and the amount of sleep he got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records = pd.read_csv(\"data/recordings-info.csv\")\n",
    "df_records.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 2. Retrieve data from file and filter to only one channel of EEG\n",
    "___\n",
    "\n",
    "This step includes:\n",
    "- Retrieving edf file\n",
    "- Excluding channels that are not EEG signals\n",
    "- Retrieving dataframe recordings information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_signal(psg_file_name, hypno_file_name):\n",
    "    \"\"\"\n",
    "    returns: mne.Raw of the whole night recording\n",
    "    \"\"\"\n",
    "    raw_data = mne.io.read_raw_edf(psg_file_name, preload=True, stim_channel=None, verbose=False)\n",
    "    annot = mne.read_annotations(hypno_file_name)\n",
    "    raw_data.set_annotations(annot, emit_warning=False)\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def drop_other_channels(raw_data, channel_to_keep):\n",
    "    \"\"\"\n",
    "    returns: mne.Raw with the two EEG channels and the signal\n",
    "        between the time the subject closed the lights and the time\n",
    "        at which the subject woke up\n",
    "    \"\"\"\n",
    "    raw_data.drop_channels([ch for ch in raw_data.info['ch_names'] if ch != channel_to_keep])\n",
    "    \n",
    "    return raw_data\n",
    "\n",
    "def get_recording_info(file_name):\n",
    "    # Each file is named according to this pattern:\n",
    "    #   SC4ssNEO-PSG.edf where SS is subject index (3:5) and N is subject night index (5:6)\n",
    "    subject_index = int(file_name[3:5])\n",
    "    subject_night = int(file_name[5:6])\n",
    "    \n",
    "    return df_records[(df_records['subject'] == subject_index) & (df_records['night'] == subject_night)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recording_info(\"SC4121E0-PSG.edf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Raw signal to epochs or matrices\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_epochs(raw_data, info):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "        mne.Epochs, where the epochs are only choosen if the subject was in bed.\n",
    "        y: np array of shape (nb_epochs,), which contains each epoch label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of seconds since file began\n",
    "    closed_lights_time = info['LightsOffSecond'].values[0]\n",
    "    woke_up_time = closed_lights_time + info['NightDuration'].values[0] + NB_EPOCHS_AWAKE_MORNING*EPOCH_DURATION\n",
    "\n",
    "    raw_data.crop(tmin=closed_lights_time, tmax=min(woke_up_time, raw_data.times[-1]))\n",
    "    \n",
    "    events, annot_event_id = mne.events_from_annotations(\n",
    "        raw_data,\n",
    "        event_id=ANNOTATIONS_EVENT_ID,\n",
    "        chunk_duration=EPOCH_DURATION,\n",
    "        verbose=False)\n",
    "    \n",
    "    # Few files do not have N3 sleep (i.e. SC4202EC-Hypnogram), so we have to filter out key-value pairs that are not in the annotations.\n",
    "    event_id = { \n",
    "        event_key: EVENT_ID[event_key] \n",
    "        for event_key in EVENT_ID\n",
    "        if EVENT_ID[event_key] in annot_event_id.values()\n",
    "    }\n",
    "    \n",
    "    epochs = mne.Epochs(\n",
    "        raw=raw_data,\n",
    "        events=events,\n",
    "        event_id=event_id,\n",
    "        tmin=0.,\n",
    "        tmax=MAX_TIME,\n",
    "        preload=True,\n",
    "        baseline=None,\n",
    "        verbose=False)\n",
    "    \n",
    "    y = np.array([event[-1] for event in epochs.events])\n",
    "    \n",
    "    return epochs, y \n",
    "\n",
    "\n",
    "def convert_to_matrices(data):\n",
    "    \"\"\"\n",
    "    data: mne.Epochs with only one EEG channel\n",
    "    \n",
    "    returns\n",
    "        - X: Matrix of input values, of size (nb_epochs, sampling_rate*epoch_length=3000)\n",
    "        - y: Vector of observation labels, of size (nb_epochs,)\n",
    "    \"\"\"\n",
    "    df = data.to_data_frame(picks=\"eeg\", long_format=True)\n",
    "    df = df.drop(columns=['ch_type', 'channel'])\n",
    "    df = df.sort_values(by=['epoch', 'time'])\n",
    "    \n",
    "    y = df[['epoch', 'condition']].drop_duplicates(keep=\"first\")['condition'].to_numpy()\n",
    "    X = np.matrix(\n",
    "        [df[df['epoch'] == epoch]['observation'].to_numpy() for epoch in df['epoch'].unique()]\n",
    "    )\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete function to apply in order to preprocess our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, current_channel, df_info, convert_to_matrix=False):\n",
    "    \"\"\"\n",
    "    data: mne.Raw \n",
    "        Instance of all of the night recording and all channels\n",
    "    current_channel: str \n",
    "        Current EEG channel\n",
    "        \n",
    "    returns\n",
    "        - X: Matrix of input values, of size (nb_epochs, sampling_rate*epoch_length=3000)\n",
    "        - y: Vector of observation labels, of size (nb_epochs,)\n",
    "    \"\"\"\n",
    "    data = drop_other_channels(data, current_channel)\n",
    "    data, y = convert_to_epochs(data, df_info)\n",
    "    \n",
    "    if not convert_to_matrix:\n",
    "        return data, y\n",
    "    \n",
    "    return convert_to_matrices(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "___\n",
    "\n",
    "\n",
    "```\n",
    "        Frequency domain\n",
    "        features                +----> Average delta band +-+\n",
    "                +-------+       |                           |\n",
    "      +-------->+  FFT  +-------+      ...                  |\n",
    "      |         +-------+       |                           |\n",
    "      |                         +----> Mean frequency  +----+\n",
    "      |                                                     |\n",
    "      |                                                     |\n",
    "      |                                                     |\n",
    "  X +-+                                                     +-> X'\n",
    "Input |                                                     |   Features\n",
    "mne.  |                         +-----> Variance +----------+   np.array\n",
    "Epochs|                         |                           |   shape:\n",
    "      |         +----------+    |                           |    (nb_epochs, nb_features)\n",
    "      +-------->+ get_data +-------------> Mean +-----------+\n",
    "                +----------+    |                           |\n",
    "                                |       ...                 |\n",
    "                                |                           |\n",
    "                                +-----> Zero cross rate ----+\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "        Time\n",
    "        domain features\n",
    "\n",
    "```\n",
    "\n",
    "We first have to define a transformer that will extract the values out of an epoch instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_epochs(epochs):\n",
    "    \"\"\"\n",
    "    epochs: mne.Epochs\n",
    "    \n",
    "    returns np array of shape (nb_epochs, sampling_rate*epoch_length)\n",
    "    \"\"\"\n",
    "    return epochs.get_data().squeeze()\n",
    "\n",
    "get_data_from_epochs_transformer = FunctionTransformer(get_data_from_epochs, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a skeleton fonction which receives a fonction that is called for every epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer(get_feature):\n",
    "    \n",
    "    def get_one_feature_per_epoch(X, get_feature):\n",
    "        \"\"\"\n",
    "        X: Input matrix (nb_epochs, sampling_rate*epoch_length)\n",
    "        get_feature: callable \n",
    "            generates one feature for each epoch\n",
    "\n",
    "        returns matrix (nb_epoch,1)\n",
    "        \"\"\"\n",
    "        return [[get_feature(epoch)] for epoch in X]\n",
    "\n",
    "    return lambda X: get_one_feature_per_epoch(X, get_feature)\n",
    "\n",
    "def get_transformer_list(get_features):\n",
    "    \n",
    "    def get_feature_list_per_epoch(X, get_features):\n",
    "        \"\"\"\n",
    "        X: Input matrix (nb_epochs, sampling_rate*epoch_length)\n",
    "        get_feature: callable \n",
    "            generates a list of features for each epoch\n",
    "\n",
    "        returns matrix (nb_epoch,nb_features)\n",
    "        \"\"\"\n",
    "        return [get_features(epoch) for epoch in X]\n",
    "\n",
    "    return lambda X: get_feature_list_per_epoch(X, get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Time domain features\n",
    "___\n",
    "\n",
    "##### a) Standard statistics\n",
    "____\n",
    "\n",
    "We extract features on the distribution of the time domain values of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_transformer = FunctionTransformer(get_transformer(np.mean), validate=True)\n",
    "std_transformer = FunctionTransformer(get_transformer(np.std), validate=True)\n",
    "skew_transformer = FunctionTransformer(get_transformer(skew), validate=True)\n",
    "kurtosis_transformer = FunctionTransformer(get_transformer(kurtosis), validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Mean crossing rate\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_crossing_rate(signal):\n",
    "    \"\"\"\n",
    "    Multiplies signal by itself shifted by one.\n",
    "    If the signal crosses the horizontal axis, the sign will be negative and vice-versa.\n",
    "    \n",
    "    Returns nb of time the signal crossed the horizontal axis\n",
    "    \"\"\"\n",
    "    return ((signal[:-1] * signal[1:]) < 0).sum()\n",
    "\n",
    "def get_mean_crossing_rate(signal):\n",
    "    return get_zero_crossing_rate(signal - np.mean(signal))\n",
    "\n",
    "mean_crossing_rate_transformer = FunctionTransformer(get_transformer(get_mean_crossing_rate), validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Hjorth parameters\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code taken from PyEEG: https://github.com/forrestbao/pyeeg\n",
    "\n",
    "def hjorth(X):\n",
    "    \"\"\" Compute Hjorth mobility and complexity of a time series from either two\n",
    "    cases below:\n",
    "        1. X, the time series of type list (default)\n",
    "        2. D, a first order differential sequence of X (if D is provided,\n",
    "           recommended to speed up)\n",
    "    In case 1, D is computed using Numpy's Difference function.\n",
    "    Notes\n",
    "    -----\n",
    "    To speed up, it is recommended to compute D before calling this function\n",
    "    because D may also be used by other functions whereas computing it here\n",
    "    again will slow down.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X\n",
    "        list\n",
    "        a time series\n",
    "    D\n",
    "        list\n",
    "        first order differential sequence of a time series\n",
    "    Returns\n",
    "    -------\n",
    "    As indicated in return line (mobility, complexity)\n",
    "    \"\"\"\n",
    "    D = np.diff(X)\n",
    "    D = D.tolist()\n",
    "\n",
    "    D.insert(0, X[0])  # pad the first difference\n",
    "    D = np.array(D)\n",
    "\n",
    "    n = len(X)\n",
    "\n",
    "    M2 = float(sum(D ** 2)) / n\n",
    "    TP = sum(np.array(X) ** 2)\n",
    "    M4 = 0\n",
    "    for i in range(1, len(D)):\n",
    "        M4 += (D[i] - D[i - 1]) ** 2\n",
    "    M4 = M4 / n\n",
    "\n",
    "    # Hjorth Mobility and Complexity\n",
    "    mobility = np.sqrt(M2 / TP)\n",
    "    complexity = np.sqrt(\n",
    "        float(M4) * TP / M2 / M2\n",
    "    )\n",
    "    return [mobility, complexity] # np.concatenate([mobility, complexity], axis=1)\n",
    "\n",
    "hjorth_transformer = FunctionTransformer(get_transformer_list(hjorth), validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging all time domain features with `FeatureUnion`\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_domain_feature_union = FeatureUnion([\n",
    "    ('mean', mean_transformer),\n",
    "    ('std', std_transformer),\n",
    "    ('skew', skew_transformer),\n",
    "    ('kurtosis', kurtosis_transformer),\n",
    "    ('mean-crossing-rate', mean_crossing_rate_transformer),\n",
    "    ('hjorth', hjorth_transformer)\n",
    "], n_jobs=1)\n",
    "\n",
    "time_domain_pipeline = Pipeline([\n",
    "    ('epochs_to_data', get_data_from_epochs_transformer),\n",
    "    ('time_domain_features', time_domain_feature_union)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Frequency domain features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eeg_power_band(epochs):\n",
    "    \"\"\"EEG relative power band feature extraction.\n",
    "\n",
    "    This function takes an ``mne.Epochs`` object and creates EEG features based\n",
    "    on relative power in specific frequency bands that are compatible with\n",
    "    scikit-learn.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : Epochs\n",
    "        The data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array of shape [n_samples, 5]\n",
    "        Transformed data.\n",
    "    \"\"\"\n",
    "    psds, freqs = psd_welch(epochs, fmin=0.5, fmax=30.)\n",
    "    # Normalize the PSDs\n",
    "    psds /= np.sum(psds, axis=-1, keepdims=True)\n",
    "\n",
    "    X = []\n",
    "    for fmin, fmax in FREQ_BANDS_RANGE.values():\n",
    "        psds_band = psds[:, :, (freqs >= fmin) & (freqs < fmax)].mean(axis=-1)\n",
    "        X.append(psds_band.reshape(len(psds), -1))\n",
    "\n",
    "    return np.concatenate(X, axis=1)\n",
    "\n",
    "eeg_power_bands_transformer = FunctionTransformer(eeg_power_band, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_domain_feature_union = FeatureUnion([\n",
    "    ('power_band', eeg_power_bands_transformer)\n",
    "], n_jobs=1)\n",
    "# frequency_domain_pipeline = make_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Sub-band features\n",
    "___\n",
    "\n",
    "Certain features discriminate more sleep stages when they are calculated only on a particular sub-band. We will provide functions which calculates the value on each sub-band, and then divide the signal before calling these functions.\n",
    "\n",
    "##### a) Mean energy\n",
    "___\n",
    "\n",
    "Mean energy corresponds to $ME = \\frac{1}{N}\\sum_{t=0}^{N} x_t^2$, where N is the epoch length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_mean_energy(signal):\n",
    "    \"\"\"\n",
    "    signal: array of (nb_sample_per_epoch,)\n",
    "    \"\"\"\n",
    "    return np.sum(signal**2)*1e6\n",
    "\n",
    "mean_energy_transformer = FunctionTransformer(get_transformer(get_signal_mean_energy), validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply 5 IIR filters on all of our epochs in order to calculate time domain features on sub-band epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline_per_subband(subband_name: str):\n",
    "    \"\"\"\n",
    "    Constructs a pipeline to extract the specified subband related features.\n",
    "    Output:\n",
    "        sklearn.pipeline.Pipeline object containing all steps to calculate time-domain feature on the specified subband.\n",
    "    \"\"\"\n",
    "    \n",
    "    freq_range = FREQ_BANDS_RANGE[subband_name]\n",
    "    order = FREQ_BANDS_ORDERS[subband_name]\n",
    "    \n",
    "    assert len(freq_range) == 2, \"Frequency range must only have 2 elements: [lower bound frequency, upper bound frequency]\"\n",
    "    \n",
    "    bounds = [freq/NYQUIST_FREQ for freq in freq_range]\n",
    "    b, a = butter(order, bounds, btype='bandpass')\n",
    "    \n",
    "    def filter_epochs_in_specified_subband(epochs):\n",
    "        return epochs.copy().filter(\n",
    "            l_freq=bounds[0],\n",
    "            h_freq=bounds[1],\n",
    "            method='iir',\n",
    "            n_jobs=1,\n",
    "            iir_params = {\n",
    "                'a': a,\n",
    "                'b': b\n",
    "            }, verbose=False)\n",
    "        \n",
    "    return Pipeline([\n",
    "        ('filter', FunctionTransformer(filter_epochs_in_specified_subband, validate=False)),\n",
    "        ('get-values', get_data_from_epochs_transformer),\n",
    "        ('mean-energy', mean_energy_transformer),\n",
    "#         ('standard-scaler', StandardScaler()) # Because we scale our features within mean_energy fct, we currently don't have to apply the scaler.\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subband_feature_union = FeatureUnion([(\n",
    "        f\"{band_name}-filter\",\n",
    "        get_pipeline_per_subband(band_name)\n",
    "    ) for band_name in FREQ_BANDS_ORDERS.keys()], n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Complete feature extraction pipeline\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion([\n",
    "    ('time_domain', time_domain_pipeline),\n",
    "    ('frequency_domain', frequency_domain_feature_union),\n",
    "    ('subband_time_domain', subband_feature_union)\n",
    "], n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = range(6)\n",
    "NIGHT_RECORDINGS = [1, 2]\n",
    "\n",
    "subject_file_names = fetch_data(subjects=SUBJECTS, recording=NIGHT_RECORDINGS)\n",
    "\n",
    "psg_file_names = [names[0] for names in subject_file_names]\n",
    "stage_file_names = [names[1] for names in subject_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_features(recording_index):\n",
    "    \"\"\"\n",
    "    recording_index: index starting at 0..nb_files-1.\n",
    "        ** It does not corresponds to the file indexes if we don't include the first files in the subjects range. **\n",
    "    Returns features X in a vector of (nb_epochs, nb_features)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Calculating for file #\", recording_index)\n",
    "        df_info = get_recording_info(psg_file_names[recording_index][-16:])\n",
    "        raw_data = fetch_signal(psg_file_names[recording_index], stage_file_names[recording_index])\n",
    "        features_file = []\n",
    "\n",
    "        for channel in EEG_CHANNELS:\n",
    "            X_file_channel, y_file_channel = preprocess(raw_data.copy(), channel, df_info)\n",
    "            X_features = feature_union.fit_transform(X_file_channel)\n",
    "            features_file.append(X_features)\n",
    "\n",
    "            print(f\"Done extracting {X_features.shape[1]} features on {X_features.shape[0]} epochs for {channel} for file {psg_file_names[recording_index][-16:]}\\n\")\n",
    "            assert X_features.shape[0] == len(y_file_channel), \"Features and labels must have the same number of epochs\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR: for file \", psg_file_names[recording_index])\n",
    "        raise e\n",
    "\n",
    "    # Only returns y one time, because both channels refer to the same epochs\n",
    "    return np.hstack(tuple(features_file)), y_file_channel\n",
    "\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    observations = pool.map(get_features, range(len(psg_file_names)))\n",
    "    X, y = zip(*observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack(X)\n",
    "y = np.hstack(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KFOLDS = 5\n",
    "\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=NB_KFOLDS).split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_test_pred = classifier.predict(X_test)\n",
    "    accuracies.append(round(accuracy_score(y_test, y_test_pred),2))\n",
    "    f1_scores.append(f1_score(y_test, y_test_pred, average=\"micro\"))\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_test_pred))\n",
    "    \n",
    "    print(classification_report(y_test, y_test_pred, target_names=EVENT_ID.keys()))\n",
    "\n",
    "print(f\"\\n\\nAccuracies accross {NB_KFOLDS} folds: {accuracies}\")\n",
    "print(f\"Mean F1-score: {np.mean(f1_scores):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
