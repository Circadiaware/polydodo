{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep stage classification: Random Forest & Hidden Markov Model\n",
    "____\n",
    "\n",
    "This model aims to classify sleep stages based on two EEG channel. We will use the features extracted in the `pipeline.ipynb` notebook as the input to a Random Forest. The output of this model will then be used as the input of a HMM. We will implement our HMM the same as in this paper (Malafeev et al., « Automatic Human Sleep Stage Scoring Using Deep Neural Networks »)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure parent folder is in PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     GroupKFold,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             cohen_kappa_score,\n",
    "                             make_scorer)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "from hmmlearn.hmm import MultinomialHMM\n",
    "from constants import (SLEEP_STAGES_VALUES,\n",
    "                       N_STAGES,\n",
    "                       EPOCH_DURATION)\n",
    "from model_utils import (print_hypnogram,\n",
    "                         train_test_split_one_subject,\n",
    "                         train_test_split_according_to_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of the subject information and night information in the X matrix\n",
    "SUBJECT_IDX = 0 \n",
    "NIGHT_IDX = 1\n",
    "USE_CONTINUOUS_AGE = False\n",
    "DOWNSIZE_SET = False\n",
    "TEST_SET_SUBJECTS = [0.0, 24.0, 49.0, 71.0]\n",
    "\n",
    "if USE_CONTINUOUS_AGE:\n",
    "    X_file_name = \"../data/x_features-age-continuous.npy\"\n",
    "    y_file_name = \"../data/y_observations-age-continuous.npy\"\n",
    "else:\n",
    "    X_file_name = \"../data/x_features.npy\"\n",
    "    y_file_name = \"../data/y_observations.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = np.load(X_file_name, allow_pickle=True)\n",
    "y_init = np.load(y_file_name, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = np.vstack(X_init)\n",
    "y_init = np.hstack(y_init)\n",
    "print(X_init.shape)\n",
    "print(y_init.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of subjects: \", np.unique(X_init[:,SUBJECT_IDX]).shape[0]) # Some subject indexes are skipped, thus total number is below 83 (as we can see in https://physionet.org/content/sleep-edfx/1.0.0/)\n",
    "print(\"Number of nights: \", len(np.unique([f\"{int(x[0])}-{int(x[1])}\" for x in X_init[:,SUBJECT_IDX:NIGHT_IDX+1]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsizing sets\n",
    "___\n",
    "\n",
    "We will use the same set for all experiments. It includes the first 20 subjects, and excludes the 13th, because it only has one night.\n",
    "\n",
    "The last subject will be put in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNSIZE_SET:\n",
    "    # Filtering to only keep first 20 subjects\n",
    "    X_20 = X_init[np.isin(X_init[:,SUBJECT_IDX], range(20))]\n",
    "    y_20 = y_init[np.isin(X_init[:,SUBJECT_IDX], range(20))]\n",
    "\n",
    "    # Exclude the subject with only one night recording (13th)\n",
    "    MISSING_NIGHT_SUBJECT = 13\n",
    "\n",
    "    X = X_20[X_20[:,SUBJECT_IDX] != MISSING_NIGHT_SUBJECT]\n",
    "    y = y_20[X_20[:,SUBJECT_IDX] != MISSING_NIGHT_SUBJECT]\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "else:\n",
    "    X = X_init\n",
    "    y = y_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of subjects: \", np.unique(X[:,SUBJECT_IDX]).shape[0]) # Some subject indexes are skipped, thus total number is below 83 (as we can see in https://physionet.org/content/sleep-edfx/1.0.0/)\n",
    "print(\"Subjects available: \", np.unique(X[:,SUBJECT_IDX]))\n",
    "print(\"Number of nights: \", len(np.unique([f\"{int(x[0])}-{int(x[1])}\" for x in X[:,SUBJECT_IDX:NIGHT_IDX+1]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation and test sets\n",
    "___\n",
    "\n",
    "If we downsize the dataset, the test set will only contain the two nights recording of the last subject (no 19) will be the test set. The rest will be the train and validation sets.\n",
    "\n",
    "If we did not downsize the dataset, we will randomly pick a subject from each age group to be in the test set. Both nights (if there are two) are placed in the test set so that the classifier does not train on any recordings from a subject placed in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNSIZE_SET:\n",
    "    X_test, X_train_valid, y_test, y_train_valid = train_test_split_one_subject(X, y)\n",
    "else:\n",
    "    X_test, X_train_valid, y_test, y_train_valid = train_test_split_according_to_age(X,\n",
    "                                                                                     y,\n",
    "                                                                                     subjects_test=TEST_SET_SUBJECTS,\n",
    "                                                                                     use_continuous_age=USE_CONTINUOUS_AGE)\n",
    "    \n",
    "print(X_test.shape, X_train_valid.shape, y_test.shape, y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest validation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KFOLDS = 5\n",
    "NB_CATEGORICAL_FEATURES = 2\n",
    "NB_FEATURES = 48\n",
    "\n",
    "CLASSIFIER_PIPELINE_KEY = 'classifier'\n",
    "\n",
    "def get_random_forest_model():\n",
    "    return Pipeline([\n",
    "        ('scaling', ColumnTransformer([\n",
    "            ('pass-through-categorical', 'passthrough', list(range(NB_CATEGORICAL_FEATURES))),\n",
    "            ('scaling-continuous', StandardScaler(copy=False), list(range(NB_CATEGORICAL_FEATURES,NB_FEATURES)))\n",
    "        ])),\n",
    "        (CLASSIFIER_PIPELINE_KEY, RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42, # enables deterministic behaviour\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cross validation, we will use the `GroupKFold` technique. For each fold, we make sure to train and validate on different subjects, to avoid overfitting over subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accuracies = []\n",
    "macro_f1_scores = []\n",
    "weighted_f1_scores = []\n",
    "kappa_agreements = []\n",
    "emission_matrix = np.zeros((N_STAGES,N_STAGES))\n",
    "\n",
    "for train_index, valid_index in GroupKFold(n_splits=5).split(X_train_valid, groups=X_train_valid[:,SUBJECT_IDX]):\n",
    "    # We drop the subject and night indexes\n",
    "    X_train, X_valid = X_train_valid[train_index, 2:], X_train_valid[valid_index, 2:]\n",
    "    y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "    \n",
    "    # Scaling features and model training\n",
    "    training_pipeline = get_random_forest_model()\n",
    "    training_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation\n",
    "    y_valid_pred = training_pipeline.predict(X_valid)\n",
    "\n",
    "    print(\"----------------------------- FOLD RESULTS --------------------------------------\\n\")\n",
    "    current_kappa = cohen_kappa_score(y_valid, y_valid_pred)\n",
    "\n",
    "    print(\"TRAIN:\", train_index, \"VALID:\", valid_index, \"\\n\\n\")\n",
    "    print(confusion_matrix(y_valid, y_valid_pred), \"\\n\")\n",
    "    print(classification_report(y_valid, y_valid_pred, target_names=SLEEP_STAGES_VALUES.keys()), \"\\n\")\n",
    "    print(\"Agreement score (Cohen Kappa): \", current_kappa, \"\\n\")\n",
    "    \n",
    "    accuracies.append(round(accuracy_score(y_valid, y_valid_pred),2))\n",
    "    macro_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"macro\"))\n",
    "    weighted_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"weighted\"))\n",
    "    kappa_agreements.append(current_kappa)\n",
    "    \n",
    "    for y_pred, y_true in zip(y_valid_pred, y_valid):\n",
    "        emission_matrix[y_true, y_pred] += 1\n",
    "\n",
    "emission_matrix = emission_matrix / emission_matrix.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(accuracies):0.2f} ± {np.std(accuracies):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(macro_f1_scores):0.2f} ± {np.std(macro_f1_scores):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(weighted_f1_scores):0.2f} ± {np.std(weighted_f1_scores):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(kappa_agreements):0.2f} ± {np.std(kappa_agreements):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation results\n",
    "___\n",
    "\n",
    "### Dimension reduction\n",
    "___\n",
    "\n",
    "*Definitions from scikit learn*:\n",
    "\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "We will compare each method and keep the one with the best results.\n",
    "\n",
    "#### 1. Linear discriminant analysis\n",
    "___\n",
    "\n",
    "LDA only allows `n_components` between 1 and `min(n_classes - 1, n_features)`. We will then **reduce our 48 features to 4 components**.\n",
    "\n",
    "A particularity of LDA is that it has pratically no hyperparameters to fix (except the number of components, which is limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_model_with_dim_reduction(dimension_reducer):\n",
    "    return Pipeline([\n",
    "        ('scaling', ColumnTransformer([\n",
    "            ('pass-through-categorical', 'passthrough', list(range(NB_CATEGORICAL_FEATURES))),\n",
    "            ('scaling-continuous', StandardScaler(copy=False), list(range(NB_CATEGORICAL_FEATURES,NB_FEATURES)))\n",
    "        ])),\n",
    "        ('dimension_reduction', dimension_reducer),\n",
    "        (CLASSIFIER_PIPELINE_KEY, RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42, # enables deterministic behaviour\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_cv_iterator(n_splits=2):\n",
    "    return GroupKFold(n_splits=n_splits).split(\n",
    "        X_train_valid, groups=X_train_valid[:,SUBJECT_IDX]\n",
    "    )\n",
    "\n",
    "clf = get_random_forest_model_with_dim_reduction(LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(\n",
    "    estimator=clf,\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    groups=X_train_valid[:,SUBJECT_IDX],\n",
    "    scoring={\n",
    "        \"agreement\": make_scorer(cohen_kappa_score),\n",
    "        \"accuracy\": 'accuracy',\n",
    "        \"f1-score-macro\": 'f1_macro',\n",
    "        \"f1-score-weighted\": 'f1_weighted',\n",
    "    },\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(scores['test_accuracy']):0.2f} ± {np.std(scores['test_accuracy']):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(scores['test_f1-score-macro']):0.2f} ± {np.std(scores['test_f1-score-macro']):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(scores['test_f1-score-weighted']):0.2f} ± {np.std(scores['test_f1-score-weighted']):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(scores['test_agreement']):0.2f} ± {np.std(scores['test_agreement']):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Principal component analysis\n",
    "___\n",
    "\n",
    "It is a non supervised dimension reduction method. In that sense, it can lead to worst results than LDA. On the other side, we can set the `n_components` to any value between 1 and `n_features`, which allows us to control the information that we leave out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = get_random_forest_model_with_dim_reduction(PCA())\n",
    "clf.fit(X_train_valid,y_train_valid)\n",
    "np.sum(clf[1].explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_of_components_included in [1,2,4,10,16,20,30]:\n",
    "    ratio_explained = np.sum(clf[1].explained_variance_ratio_[:number_of_components_included])\n",
    "    print(f\"{number_of_components_included} components included: {ratio_explained:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try for 4 components (to compare with LDA), 16 components, which corresponds to a reasonable ratio explained (>99%), and 30, which corresponds of almost all of the explained ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score_with_pca(n_components):\n",
    "    return cross_validate(\n",
    "        estimator=get_random_forest_model_with_dim_reduction(PCA(n_components=n_components)),\n",
    "        X=X_train_valid,\n",
    "        y=y_train_valid,\n",
    "        groups=X_train_valid[:,SUBJECT_IDX],\n",
    "        scoring={\n",
    "            \"agreement\": make_scorer(cohen_kappa_score),\n",
    "            \"accuracy\": 'accuracy',\n",
    "            \"f1-score-macro\": 'f1_macro',\n",
    "            \"f1-score-weighted\": 'f1_weighted',\n",
    "        },\n",
    "        cv=get_cv_iterator(n_splits=5),\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_score_with_pca(n_components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(scores['test_accuracy']):0.2f} ± {np.std(scores['test_accuracy']):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(scores['test_f1-score-macro']):0.2f} ± {np.std(scores['test_f1-score-macro']):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(scores['test_f1-score-weighted']):0.2f} ± {np.std(scores['test_f1-score-weighted']):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(scores['test_agreement']):0.2f} ± {np.std(scores['test_agreement']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_score_with_pca(n_components=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(scores['test_accuracy']):0.2f} ± {np.std(scores['test_accuracy']):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(scores['test_f1-score-macro']):0.2f} ± {np.std(scores['test_f1-score-macro']):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(scores['test_f1-score-weighted']):0.2f} ± {np.std(scores['test_f1-score-weighted']):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(scores['test_agreement']):0.2f} ± {np.std(scores['test_agreement']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_score_with_pca(n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(scores['test_accuracy']):0.2f} ± {np.std(scores['test_accuracy']):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(scores['test_f1-score-macro']):0.2f} ± {np.std(scores['test_f1-score-macro']):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(scores['test_f1-score-weighted']):0.2f} ± {np.std(scores['test_f1-score-weighted']):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(scores['test_agreement']):0.2f} ± {np.std(scores['test_agreement']):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_score_with_pca(n_components=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean accuracy          : {np.mean(scores['test_accuracy']):0.2f} ± {np.std(scores['test_accuracy']):0.3f}\")\n",
    "print(f\"Mean macro F1-score    : {np.mean(scores['test_f1-score-macro']):0.2f} ± {np.std(scores['test_f1-score-macro']):0.3f}\")\n",
    "print(f\"Mean weighted F1-score : {np.mean(scores['test_f1-score-weighted']):0.2f} ± {np.std(scores['test_f1-score-weighted']):0.3f}\")\n",
    "print(f\"Mean Kappa's agreement : {np.mean(scores['test_agreement']):0.2f} ± {np.std(scores['test_agreement']):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we resume all our results, which we tested upon 5 folds, we have the following:\n",
    "\n",
    "|  Score             |  without      | LDA           | PCA (n_comp=4) | PCA (n_comp=16) | PCA (n_comp=30) | PCA (n_comp=40) |\n",
    "|--------------------|---------------|---------------|----------------|-----------------|-----------------|-----------------|\n",
    "| accuracy           | 0.72 ± 0.030  | 0.68 ± 0.032  | 0.54 ± 0.020   |  0.69 ± 0.031   |  0.70 ± 0.026   |  0.71 ± 0.027   |\n",
    "|  macro F1-score    | 0.65 ± 0.027  |  0.60 ± 0.026 | 0.48 ± 0.008   |  0.61 ± 0.032   |  0.61 ± 0.029   |  0.62 ± 0.030   |\n",
    "|  weighted F1-score | 0.71 ± 0.029  |  0.67 ± 0.029 | 0.53 ± 0.015   |  0.67 ± 0.032   |  0.68 ± 0.029   |  0.68 ± 0.029   |\n",
    "|  Kappa's agreement | 0.61 ± 0.043  |  0.56 ± 0.043 | 0.36 ± 0.024   |  0.57 ± 0.042   |  0.58 ± 0.038   |  0.59 ± 0.038   |\n",
    "\n",
    "Considering that:\n",
    "- the number of features currently defined, namely 48, does not considerably slows down our prediction (about 228 ms)\n",
    "- the scores obtained with LDA and PCA are significally lower than without applying dimension reduction\n",
    "- we prefer to have a slower model to train (currently trainable under about 2 minutes) than to lose some accuracy\n",
    "- our current classifier still works when there is multicolinearity between features (PCA and LDA should have otherwise been used)\n",
    "\n",
    "We will not apply dimension reduction on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest hyperparameters\n",
    "___\n",
    "\n",
    "We have fixed the same hyperparameters for the other categories:\n",
    "- `StandardScaler` for all continuous features\n",
    "- Training on all the dataset (83 subjects)\n",
    "- Test set contains the following subjects: `[0.0, 24.0, 49.0, 71.0]`\n",
    "- No postprocessing step\n",
    "- RF with its default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hyperparam_tuning_results(search_cv_results):\n",
    "\n",
    "    for idx, rank in enumerate(search_cv_results['rank_test_score']):\n",
    "        current_param = search_cv_results['params'][idx]\n",
    "        score_mean = search_cv_results['mean_test_score'][idx]\n",
    "        score_uncertainty = search_cv_results['std_test_score'][idx]\n",
    "        print(f\"{rank}. Parameter {current_param} has a score of {score_mean:0.4f} ± {score_uncertainty:0.3f}\")\n",
    "        \n",
    "def evaluate_hyperparams_grid(grid_param):\n",
    "    search = GridSearchCV(\n",
    "        estimator=get_random_forest_model(),\n",
    "        param_grid=grid_param,\n",
    "        scoring=make_scorer(cohen_kappa_score),\n",
    "        cv=get_cv_iterator(),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_valid[:,2:], y_train_valid)\n",
    "\n",
    "    print_hyperparam_tuning_results(search.cv_results_)\n",
    "    \n",
    "def evaluate_hyperparams_randomized(params):\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=get_random_forest_model(),\n",
    "        param_distributions=params,\n",
    "        scoring=make_scorer(cohen_kappa_score),\n",
    "        cv=get_cv_iterator(),\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_train_valid[:,2:], y_train_valid)\n",
    "\n",
    "    print_hyperparam_tuning_results(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. class_weight: balanced vs none\n",
    "___\n",
    "\n",
    "Weights associated with classes.\n",
    "\n",
    "Because our dataset has unbalanced data (more N2, less N3 per example), we want to check if it gives better result to have a balanced vs not balanced training set.\n",
    "\n",
    "> The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))` (Source: sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__class_weight\": [None, \"balanced\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  Value      |  Score          |\n",
    "|-------------|-----------------|\n",
    "| `None`      | 0.6153 ± 0.003  |\n",
    "|  `balanced` |  0.6114 ± 0.002 |\n",
    "\n",
    "The chosen value for `class_weight` is then `None`.\n",
    "\n",
    "#### 2. n_estimators\n",
    "___\n",
    "\n",
    "Definition: the number of trees in the forest.\n",
    "\n",
    "It's usually better to pick a higher n_estimators, but it takes slower to train and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_randomized({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__n_estimators\": np.linspace(50, 1000, 8, dtype=\"int\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First experience**: `np.linspace(50, 1000, 8, dtype=\"int\")`\n",
    "\n",
    "|Rank|  n_estimators |  Score          |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 592           |  0.6201 ± 0.005 |\n",
    "|2   |  728          |  0.6200 ± 0.006 |\n",
    "|3   |  864          |  0.6200 ± 0.005 |\n",
    "|4   |  1000         |  0.6193 ± 0.005 |\n",
    "|5   |  457          |  0.6191 ± 0.006 |\n",
    "|6   |  321          |  0.6188 ± 0.005 |\n",
    "|7   |  185          |  0.6177 ± 0.004 |\n",
    "|8   |  50           |  0.6091 ± 0.004 |\n",
    "\n",
    "Total running time: 57min 4s\n",
    "\n",
    "We can see that the range with the best scores is inbetween 457 and 728 (with 592 in the middle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. max_features\n",
    "___\n",
    "\n",
    "Definition: The number of features to consider when looking for the best split.\n",
    "\n",
    "By default (`auto` or `sqrt`), the number of features considered is `sqrt(nb_features)`. In our case, it corresponds to `sqrt(48) = 6 features`. We can also define a percentage of features to consider (by passing a `float`), a number (by passing an `int`), or look at all of them (by passing `None`).\n",
    "\n",
    "As with `n_estimators`, it is better to have a higher `max_features`, but it slows down training and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_randomized({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__max_features\": [4, 5, 6, 7]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First experience**: `[\"auto\", 10, 20, 40, None]`\n",
    "\n",
    "|Rank|  max_features |  Test score     |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 'auto' = 6    |  0.6153 ± 0.003 |\n",
    "|2   |  10           |  0.6144 ± 0.004 |\n",
    "|3   |  20           |  0.6136 ± 0.005 |\n",
    "|4   |  40           |  0.6073 ± 0.000 |\n",
    "|5   |  None = 48    |  0.6049 ± 0.002 |\n",
    "\n",
    "Total running time: 21min 36s\n",
    "\n",
    "We can see that the test score is better when max_features is lower, because we can assume that we overfit our training set.\n",
    "\n",
    "**Second experience**: `[4, 5, 6, 7]`\n",
    "\n",
    "|Rank|  max_features |  Test score     |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 7             |  0.6153 ± 0.003 |\n",
    "|2   | 'auto' = 6    |  0.6153 ± 0.003 |\n",
    "|3   |  5            |  0.6145 ± 0.005 |\n",
    "|4   |  4            |  0.6142 ± 0.004 |\n",
    "\n",
    "Total running time: 5min 27s \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. max_depth\n",
    "___\n",
    "\n",
    "Definition of **max_depth**: Maximum depth of every trees in the forest (default=`None`)\n",
    "\n",
    "If max_depth is `None`, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "The maximum depth should not be too big, there would otherwise be overfitting on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_randomized({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__max_depth\": [*np.linspace(20, 40, 6, dtype=\"int\"), None]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First experience**: `[*np.linspace(8, 40, 6, dtype=\"int\"), None]`\n",
    "\n",
    "|Rank|  max_depth    |  Test score     |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 33            |  0.6164 ± 0.004 |\n",
    "|2   |  40           |  0.6160 ± 0.003 |\n",
    "|3   |  20           |  0.6160 ± 0.005 |\n",
    "|4   |  27           |  0.6155 ± 0.003 |\n",
    "|5   |  None         |  0.6153 ± 0.003 |\n",
    "|6   |  14           |  0.6134 ± 0.005 |\n",
    "|7   |  8            |  0.5944 ± 0.009 |\n",
    "\n",
    "\n",
    "Total running time: 8min 47s\n",
    "\n",
    "We can see that the range with the best scores is in between 20 and 40.\n",
    "\n",
    "**Second experience**: `[*np.linspace(20, 40, 6, dtype=\"int\"), None]` \n",
    "\n",
    "|Rank|  max_depth    |  Test score     |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 24            |  0.6177 ± 0.004 |\n",
    "|2   |  36           |  0.6167 ± 0.004 |\n",
    "|3   |  32           |  0.6162 ± 0.004 |\n",
    "|4   |  40           |  0.6160 ± 0.003 |\n",
    "|5   |  20           |  0.6160 ± 0.005 |\n",
    "|6   | None          |  0.6153 ± 0.003 |\n",
    "|7   |  28           |  0.6151 ± 0.003 |\n",
    "\n",
    "Total running time: 9min 38s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. min_samples_split and min_samples_leaf\n",
    "___\n",
    "\n",
    "Definition of **min_samples_split**: Minimum number of samples required to split an **internal node** (default=2)\n",
    "\n",
    "Definitionof **min_samples_leaf**: Minimum number of samples required to be at a **leaf node** (default=1). A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. \n",
    "\n",
    "The same reasoning can be applied to min_samples_split, where it shouldn't be too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_randomized({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__min_samples_split\": [2, 5, 10],\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__min_samples_leaf\": [1, 2]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First experience**: `min_samples_split\": [2, 5, 10], min_samples_leaf\": [1, 2]`\n",
    "\n",
    "|Rank|  min_samples_split    | min_samples_leaf | Test score     |\n",
    "|----|-----------------------|------------------|----------------|\n",
    "|1   | 10                    |    1             | 0.6171 ± 0.005 |\n",
    "|2   |  10                   |      2           | 0.6169 ± 0.006 |\n",
    "|3   |  2                    |         2        | 0.6167 ± 0.005 |\n",
    "|4   |  5                    |    1             | 0.6160 ± 0.005 |\n",
    "|5   |  5                    |     2            | 0.6160 ± 0.005 |\n",
    "|6   |  2                    |      1           | 0.6153 ± 0.003 |\n",
    "\n",
    "\n",
    "Total running time: 10min 1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Combining all best parameters to find the best combinaison\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_randomized({\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__n_estimators\": [592],\n",
    "    f\"{CLASSIFIER_PIPELINE_KEY}__max_depth\": [24, 36],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First experiment**\n",
    "\n",
    "|Rank|  n_estimators         | max_depth        | min_samples_leaf | Test score     |\n",
    "|----|-----------------------|------------------|------------------|----------------|\n",
    "|1   | 550                   |    24            |    2             | 0.6198 ± 0.006 |\n",
    "|2   | 600                   |    24            |      2           | 0.6198 ± 0.006 |\n",
    "|3   | 550                   |    36            |         2        | 0.6195 ± 0.006 |\n",
    "|4   | 450                   |    36            |    1             | 0.6195 ± 0.006 |\n",
    "|5   | 450                   |    24            |     2            | 0.6195 ± 0.006 |\n",
    "|6   | 500                   |    36            |      2           | 0.6190 ± 0.006 |\n",
    "|7   | 600                   |    24            |      1           | 0.6190 ± 0.006 |\n",
    "|8   | 450                   |    36            |      2           | 0.6190 ± 0.006 |\n",
    "|9   | 450                   |    24            |      1           | 0.6188 ± 0.006 |\n",
    "|10  | 550                   |    24            |      1           | 0.6186 ± 0.006 |\n",
    "\n",
    "Constants hyperparameters:\n",
    "    class_weight: None,\n",
    "    max_features: 7,\n",
    "    min_samples_split: 10\n",
    "\n",
    "Total running time: 1h 17min 59s\n",
    "\n",
    "We can see that the best combination has a test score lower than the the configuration of 592 estimators and the default configuration for the other hyperparameters.\n",
    "\n",
    "**Second experiment**: n_estimators: 592, max_depth: [24, 36]\n",
    "\n",
    "|Rank| max_depth        | Test score     |\n",
    "|----|------------------|----------------|\n",
    "|1   |    24            | 0.6203 ± 0.006 |\n",
    "|2   |    36            | 0.6201 ± 0.006 |\n",
    "\n",
    "We have the best, up to date, test score with n_estimators: 592, max_depth: 24, and other hyperparameters set to their default value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest training and testing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "testing_pipeline = Pipeline([\n",
    "    ('scaling', ColumnTransformer([\n",
    "        ('pass-through-categorical', 'passthrough', list(range(NB_CATEGORICAL_FEATURES))),\n",
    "        ('scaling-continuous', StandardScaler(copy=False), list(range(NB_CATEGORICAL_FEATURES,NB_FEATURES)))\n",
    "    ])),\n",
    "    (CLASSIFIER_PIPELINE_KEY, RandomForestClassifier(\n",
    "        n_estimators=592,\n",
    "        max_depth=24,\n",
    "        random_state=42, # enables deterministic behaviour\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "testing_pipeline.fit(X_train_valid[:, 2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_indexes = [\n",
    "    (idx, round(importance,4))\n",
    "    for idx, importance in enumerate(testing_pipeline.steps[1][1].feature_importances_)\n",
    "]\n",
    "feature_importance_indexes.sort(reverse=True, key=lambda x: x[1])\n",
    "\n",
    "category_feature_range = np.array([2, 3]) - 2\n",
    "time_domaine_feature_range = np.array([4, 5, 6, 7, 8, 9, 10, 27, 28, 29, 30, 31, 32, 33]) - 2\n",
    "freq_domain_feature_range = np.array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]) - 2\n",
    "subband_domain_feature_range = np.array([22, 23, 24, 25, 26, 45, 46, 47, 48, 49]) - 2\n",
    "fpz_cz_feature_range = np.array(range(2, 25))\n",
    "pz_oz_feature_range = np.array(range(25, 48))\n",
    "\n",
    "def get_feature_range_importance(indexes):\n",
    "    return np.sum([feature[1] for feature in feature_importance_indexes if feature[0] in indexes])\n",
    "\n",
    "print(f\"Categorical features:         {category_feature_range}\")\n",
    "print(f\"Time domain features:         {time_domaine_feature_range}\")\n",
    "print(f\"Frequency domain features:    {freq_domain_feature_range}\")\n",
    "print(f\"Subband time domain features: {subband_domain_feature_range}\\n\")\n",
    "\n",
    "print(f\"Top 5 features:    {[feature for feature in feature_importance_indexes[:5]]}\")\n",
    "print(f\"Bottom 5 features: {[feature for feature in feature_importance_indexes[-5:]]}\\n\")\n",
    "\n",
    "print(f\"Fpz-Cz feature importances:   {get_feature_range_importance(fpz_cz_feature_range):.4f}\")\n",
    "print(f\"Pz-Oz feature importances:    {get_feature_range_importance(pz_oz_feature_range):.4f}\\n\")\n",
    "\n",
    "print(f\"Category feature importances:            {get_feature_range_importance([0,1]):.4f}\")\n",
    "print(f\"Time domain feature importances:         {get_feature_range_importance(time_domaine_feature_range):.4f}\")\n",
    "print(f\"Frequency domain feature importances:    {get_feature_range_importance(freq_domain_feature_range):.4f}\")\n",
    "print(f\"Subband time domain feature importances: {get_feature_range_importance(subband_domain_feature_range):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = testing_pipeline.predict(X_test[:,2:])\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Model Markov\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hmm_matrices(y, subject_night):\n",
    "    transition_matrix = np.zeros((N_STAGES,N_STAGES))\n",
    "    start_matrix = np.zeros((N_STAGES))\n",
    "\n",
    "    for night in groupby(zip(y, subject_night), key=lambda x: f\"subject{int(x[1][0])}-night{int(x[1][1])}\"):\n",
    "        print(f\"Computing file: {night[0]}\")\n",
    "        current_y = np.array([x[0] for x in night[1]])\n",
    "        start_matrix[current_y[0]] += 1\n",
    "\n",
    "        for transition in zip(current_y[:-1], current_y[1:]):\n",
    "            transition_matrix[transition[0], transition[1]] += 1\n",
    "            \n",
    "    transition_matrix = transition_matrix/transition_matrix.sum(axis=1, keepdims=True)\n",
    "    start_matrix = start_matrix/start_matrix.sum()\n",
    "    \n",
    "    return transition_matrix, start_matrix\n",
    "    \n",
    "transition_matrix, start_matrix = compute_hmm_matrices(y_train_valid, X_train_valid[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = MultinomialHMM(n_components=N_STAGES)\n",
    "\n",
    "hmm_model.transmat_ = transition_matrix\n",
    "hmm_model.startprob_ = start_matrix\n",
    "hmm_model.emissionprob_ = emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hmm_pred = hmm_model.predict(y_test_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hmm_pred))\n",
    "\n",
    "print(classification_report(y_test, y_hmm_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_hmm_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test subjects are subjects: \", np.unique(X_test[:,0]))\n",
    "print(\"BEFORE HMM\")\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "for test_subject in np.unique(X_test[:,0]):\n",
    "    test_subject_indexes = [idx for idx, elem in enumerate(X_test) if elem[0] == test_subject]\n",
    "    \n",
    "    for night_idx in np.unique(X_test[test_subject_indexes,1]):\n",
    "        test_night_subject_indexes = [\n",
    "            idx for idx, elem in enumerate(X_test)\n",
    "            if elem[0] == test_subject and elem[1] == night_idx]\n",
    "        hypnograms = [\n",
    "            y_test[test_night_subject_indexes],\n",
    "            y_test_pred[test_night_subject_indexes]\n",
    "        ]\n",
    "        \n",
    "        print_hypnogram(hypnograms,\n",
    "                        labels=[\"scored\", \"predicted\"],\n",
    "                        subject=test_subject,\n",
    "                        night=night_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test subjects are subjects: \", np.unique(X_test[:,0]))\n",
    "print(\"AFTER HMM\")\n",
    "\n",
    "for test_subject in np.unique(X_test[:,0]):\n",
    "    test_subject_indexes = [idx for idx, elem in enumerate(X_test) if elem[0] == test_subject]\n",
    "    \n",
    "    for night_idx in np.unique(X_test[test_subject_indexes,1]):\n",
    "        test_night_subject_indexes = [\n",
    "            idx for idx, elem in enumerate(X_test)\n",
    "            if elem[0] == test_subject and elem[1] == night_idx]\n",
    "        hypnograms = [\n",
    "            y_test[test_night_subject_indexes],\n",
    "            y_hmm_pred[test_night_subject_indexes]\n",
    "        ]\n",
    "        \n",
    "        print_hypnogram(hypnograms,\n",
    "                        labels=[\"scored\", \"predicted with HMM\"],\n",
    "                        subject=test_subject,\n",
    "                        night=night_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median filter\n",
    "___\n",
    "\n",
    "In order to compare the HMM postprocessing step, we will apply a median filter to the output of the RF.\n",
    "\n",
    "We apply a median filter with a 3 element sized kernel. It is applied to each nights sleep seperatly. We do this because short transitions are not common in sleep patterns:\n",
    "\n",
    "> The final stage includes median filtration of short-term transitions. It is well known that short-term transitions in human sleep are impossible. Short-term jumps in hypnogram are evidence of transitory stage. Averaging for 2-3 min allows the curve structure to be smoothed and sleep structure to be resolved. \n",
    "\n",
    "Source: Doroshenkov, L. G., V. A. Konyshev, et S. V. Selishchev. « Classification of Human Sleep Stages Based on EEG Processing Using Hidden Markov Models ». Biomedical Engineering 41, nᵒ 1 (janvier 2007): 25‑28. https://doi.org/10.1007/s10527-007-0006-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL_SIZE=3\n",
    "y_medfilt_pred = np.zeros(y_test.shape[0])\n",
    "\n",
    "for test_subject in np.unique(X_test[:,0]):\n",
    "    test_subject_indexes = [idx for idx, elem in enumerate(X_test) if elem[0] == test_subject]\n",
    "    \n",
    "    for night_idx in np.unique(X_test[test_subject_indexes,1]):\n",
    "        test_night_subject_indexes = [\n",
    "            idx for idx, elem in enumerate(X_test)\n",
    "            if elem[0] == test_subject and elem[1] == night_idx]\n",
    "\n",
    "        y_medfilt_pred[test_night_subject_indexes] = medfilt(y_test_pred[test_night_subject_indexes], kernel_size=KERNEL_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_medfilt_pred))\n",
    "\n",
    "print(classification_report(y_test, y_medfilt_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_medfilt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "print(\"Test subjects are subjects: \", np.unique(X_test[:,0]))\n",
    "\n",
    "for test_subject in np.unique(X_test[:,0]):\n",
    "    test_subject_indexes = [idx for idx, elem in enumerate(X_test) if elem[0] == test_subject]\n",
    "    \n",
    "    for night_idx in np.unique(X_test[test_subject_indexes,1]):\n",
    "        test_night_subject_indexes = [\n",
    "            idx for idx, elem in enumerate(X_test)\n",
    "            if elem[0] == test_subject and elem[1] == night_idx]\n",
    "        \n",
    "        hypnograms = [\n",
    "            y_test[test_night_subject_indexes],\n",
    "            y_medfilt_pred[test_night_subject_indexes]\n",
    "        ]\n",
    "        \n",
    "        print_hypnogram(hypnograms,\n",
    "                        labels=[\"scored\", \"predicted with median filter\"],\n",
    "                        subject=test_subject,\n",
    "                        night=night_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained model\n",
    "___\n",
    "\n",
    "We save the trained model with the postprocessing step, HMM. We will save only the matrix that define it. We do not need to persist the median filter postprocessing step, because it is stateless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR = \"../trained_model\"\n",
    "\n",
    "if not os.path.exists(SAVED_DIR):\n",
    "    os.mkdir(SAVED_DIR);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CONTINUOUS_AGE: \n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_RF_continous_age.joblib\")\n",
    "else:\n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_RF_TUNED_2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{SAVED_DIR}/HMM_transmat.npy\", hmm_model.transmat_)\n",
    "np.save(f\"{SAVED_DIR}/HMM_startprob.npy\", hmm_model.startprob_)\n",
    "np.save(f\"{SAVED_DIR}/HMM_emissionprob.npy\", hmm_model.emissionprob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('py3': conda)",
   "language": "python",
   "name": "python36864bitpy3conda60d5576ae6834cf0a5dc3773043eb4d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
