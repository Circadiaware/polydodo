{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep stage classification: SVC\n",
    "____\n",
    "\n",
    "This model aims to classify sleep stages based on two EEG channel. We will use the features extracted in the `pipeline.ipynb` notebook as the input to a support vector classification (SVC).\n",
    "\n",
    "We will only be looking at `LinearSVC`, because the `SVC` model implemented by scikitlearn has a quadratic time complexity. As it is stated in the docs, it may be impractical beyond tens of thousands of samples, which corresponds to our sample numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure parent folder is in PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     GroupKFold,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             cohen_kappa_score,\n",
    "                             make_scorer)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from constants import (SLEEP_STAGES_VALUES,\n",
    "                       N_STAGES,\n",
    "                       EPOCH_DURATION)\n",
    "from model_utils import (print_hypnogram,\n",
    "                         train_test_split_one_subject,\n",
    "                         train_test_split_according_to_age,\n",
    "                         evaluate_hyperparams_grid,\n",
    "                         print_results_cv,\n",
    "                         print_results_cv_scores,\n",
    "                         get_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of the subject information and night information in the X matrix\n",
    "SUBJECT_IDX = 0 \n",
    "NIGHT_IDX = 1\n",
    "USE_CONTINUOUS_AGE = False\n",
    "DOWNSIZE_SET = False\n",
    "TEST_SET_SUBJECTS = [0.0, 24.0, 49.0, 71.0]\n",
    "\n",
    "if USE_CONTINUOUS_AGE:\n",
    "    X_file_name = \"../data/x_features-age-continuous.npy\"\n",
    "    y_file_name = \"../data/y_observations-age-continuous.npy\"\n",
    "else:\n",
    "    X_file_name = \"../data/x_features.npy\"\n",
    "    y_file_name = \"../data/y_observations.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_init = np.load(X_file_name, allow_pickle=True)\n",
    "y_init = np.load(y_file_name, allow_pickle=True)\n",
    "\n",
    "X_init = np.vstack(X_init)\n",
    "y_init = np.hstack(y_init)\n",
    "print(X_init.shape)\n",
    "print(y_init.shape)\n",
    "print(\"Number of subjects: \", np.unique(X_init[:,SUBJECT_IDX]).shape[0]) # Some subject indexes are skipped, thus total number is below 83 (as we can see in https://physionet.org/content/sleep-edfx/1.0.0/)\n",
    "print(\"Number of nights: \", len(np.unique([f\"{int(x[0])}-{int(x[1])}\" for x in X_init[:,SUBJECT_IDX:NIGHT_IDX+1]])))\n",
    "print(\"Subjects available: \", np.unique(X_init[:,SUBJECT_IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_train_valid, y_test, y_train_valid = train_test_split_according_to_age(\n",
    "    X_init,\n",
    "    y_init,\n",
    "    use_continuous_age=USE_CONTINUOUS_AGE,\n",
    "    subjects_test=TEST_SET_SUBJECTS)\n",
    "    \n",
    "print(X_test.shape, X_train_valid.shape, y_test.shape, y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC validation\n",
    "____\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KFOLDS = 5\n",
    "NB_CATEGORICAL_FEATURES = 2\n",
    "NB_FEATURES = 48\n",
    "CLASSIFIER_PIPELINE_KEY = 'classifier'\n",
    "RANDOM_STATE = 42 \n",
    "\n",
    "def get_cv_iterator(n_splits=2):\n",
    "    return GroupKFold(n_splits=n_splits).split(\n",
    "        X_train_valid, groups=X_train_valid[:,SUBJECT_IDX]\n",
    "    )\n",
    "    \n",
    "def cross_validate_with_confusion_matrix(pipeline, n_fold):\n",
    "    accuracies = []\n",
    "    macro_f1_scores = []\n",
    "    weighted_f1_scores = []\n",
    "    kappa_agreements = []\n",
    "\n",
    "    for train_index, valid_index in get_cv_iterator(n_splits=n_fold):\n",
    "        # We drop the subject and night indexes\n",
    "        X_train, X_valid = X_train_valid[train_index, 2:], X_train_valid[valid_index, 2:]\n",
    "        y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "\n",
    "        # Scaling features and model training\n",
    "        training_pipeline = pipeline\n",
    "        training_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Validation\n",
    "        y_valid_pred = training_pipeline.predict(X_valid)\n",
    "\n",
    "        print(\"----------------------------- FOLD RESULTS --------------------------------------\\n\")\n",
    "        current_kappa = cohen_kappa_score(y_valid, y_valid_pred)\n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"VALID:\", valid_index, \"\\n\\n\")\n",
    "        print(confusion_matrix(y_valid, y_valid_pred), \"\\n\")\n",
    "        print(classification_report(y_valid, y_valid_pred, target_names=SLEEP_STAGES_VALUES.keys()), \"\\n\")\n",
    "        print(\"Agreement score (Cohen Kappa): \", current_kappa, \"\\n\")\n",
    "\n",
    "        accuracies.append(round(accuracy_score(y_valid, y_valid_pred),2))\n",
    "        macro_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"macro\"))\n",
    "        weighted_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"weighted\"))\n",
    "        kappa_agreements.append(current_kappa)\n",
    "\n",
    "    print_results_cv(accuracies, macro_f1_scores, weighted_f1_scores, kappa_agreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_confusion_matrix(get_pipeline(\n",
    "    classifier=LinearSVC(\n",
    "        dual=False, # Prefer dual=False when n_samples > n_features. (documentation)\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "), n_fold=NB_KFOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CV Results:\n",
    "```\n",
    "Mean accuracy          : 0.70 ± 0.026\n",
    "Mean macro F1-score    : 0.63 ± 0.025\n",
    "Mean weighted F1-score : 0.69 ± 0.026\n",
    "Mean Kappa's agreement : 0.59 ± 0.039\n",
    "```\n",
    "\n",
    "## Validation results\n",
    "___\n",
    "\n",
    "### Dimension reduction\n",
    "___\n",
    "\n",
    "*Definitions from scikit learn*:\n",
    "\n",
    "Principal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data.\n",
    "\n",
    "Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n",
    "\n",
    "We will compare each method and keep the one with the best results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_with_dim_reduction(dim_reduction):\n",
    "    scores = cross_validate(\n",
    "        estimator=get_pipeline(\n",
    "            classifier=LinearSVC(\n",
    "                dual=False, # Prefer dual=False when n_samples > n_features. (documentation)\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=RANDOM_STATE\n",
    "            ),\n",
    "            dimension_reduction=dim_reduction\n",
    "        ),\n",
    "        X=X_train_valid,\n",
    "        y=y_train_valid,\n",
    "        groups=X_train_valid[:,SUBJECT_IDX],\n",
    "        scoring={\n",
    "            \"agreement\": make_scorer(cohen_kappa_score),\n",
    "            \"accuracy\": 'accuracy',\n",
    "            \"f1-score-macro\": 'f1_macro',\n",
    "            \"f1-score-weighted\": 'f1_weighted',\n",
    "        },\n",
    "        cv=get_cv_iterator(n_splits=5),\n",
    "        return_train_score=True,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    print_results_cv_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Linear discriminant analysis\n",
    "___\n",
    "\n",
    "LDA only allows `n_components` between 1 and `min(n_classes - 1, n_features)`. We will then **reduce our 48 features to 4 components**.\n",
    "\n",
    "A particularity of LDA is that it has pratically no hyperparameters to fix (except the number of components, which is limited)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Principal component analysis\n",
    "___\n",
    "\n",
    "It is a non supervised dimension reduction method. In that sense, it can lead to worst results than LDA. On the other side, we can set the `n_components` to any value between 1 and `n_features`, which allows us to control the information that we leave out.\n",
    "\n",
    "We've already looked at explained ratio by nb of components in the RF_HMM notebook. The results were:\n",
    "```\n",
    "1 components included: 0.9253\n",
    "2 components included: 0.9405\n",
    "4 components included: 0.9591\n",
    "10 components included: 0.9807\n",
    "16 components included: 0.9906\n",
    "20 components included: 0.9943\n",
    "30 components included: 0.9987\n",
    "```\n",
    "\n",
    "We will look at 4, 16 and 30 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we resume all our results, which we tested upon 5 folds, we have the following:\n",
    "\n",
    "|  Score             |  without      | LDA           | PCA (n_comp=4) | PCA (n_comp=16) | PCA (n_comp=30) | PCA (n_comp=35) |\n",
    "|--------------------|---------------|---------------|----------------|-----------------|-----------------|-----------------|\n",
    "|  accuracy          | 0.70 ± 0.030  |  0.69 ± 0.033 | 0.53 ± 0.018   |  0.66 ± 0.028   |  0.69 ± 0.028   |  0.70 ± 0.030   |\n",
    "|  macro F1-score    | 0.63 ± 0.027  |  0.60 ± 0.030 | 0.46 ± 0.011   |  0.58 ± 0.022   |  0.62 ± 0.027   |  0.62 ± 0.027   |\n",
    "|  weighted F1-score | 0.69 ± 0.028  |  0.67 ± 0.030 | 0.51 ± 0.011   |  0.64 ± 0.025   |  0.68 ± 0.028   |  0.69 ± 0.028   |\n",
    "|  Kappa's agreement | 0.59 ± 0.042  |  0.57 ± 0.044 | 0.37 ± 0.025   |  0.53 ± 0.037   |  0.58 ± 0.038   |  0.59 ± 0.041   |\n",
    "|  Time              | 57.1 s        |  12 s         | 12.3 s         |  16.2 s         |  30 s           |  36 s           |\n",
    "\n",
    "We will keep the PCA dimension reduction, with n_components=35, because it keeps the same accuracy, weighted F1-score and Cohen's Kappa as the model that doesn't not have dimension reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning\n",
    "___\n",
    "\n",
    "#### 1. `C`: Regularization parameter\n",
    "___\n",
    "\n",
    "Inverse of regularization strength. Like in support vector machines, smaller values specify stronger regularization. It controls the trade off between smooth decision boundary and classifying the training points correctly. Increasing C values may lead to overfitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__C\": np.linspace(1.95, 2.3, 10)\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=LinearSVC(\n",
    "            dual=False, \n",
    "            class_weight=\"balanced\",\n",
    "            random_state=RANDOM_STATE\n",
    "        ),\n",
    "        dimension_reduction=PCA(n_components=35)\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    use_randomized=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**:\n",
    "\n",
    "|Rank| C                | Test score     |\n",
    "|----|------------------|----------------|\n",
    "|1   | 2.236842105263158| 0.5903 ± 0.039 |\n",
    "|2   | 1.931578947368421| 0.5902 ± 0.039 |\n",
    "|3   | 2.389473684210526| 0.5902 ± 0.039 |\n",
    "...\n",
    "\n",
    "CPU times: user 13.1 s, sys: 998 ms, total: 14.1 s\n",
    "Wall time: 5min 28s\n",
    "\n",
    "Other values gave the same test score (`0.5902 ± 0.039`): `0.557894, 1.626315, 1.473684, 1.015789, ... 2.847368`. We then looked at the range between 1.95 and 2.3:\n",
    "\n",
    "|Rank| C                | Test score     |\n",
    "|----|------------------|----------------|\n",
    "|1   | 2.105            | 0.5903 ± 0.039 |\n",
    "|2   | 2.144            | 0.5902 ± 0.039 |\n",
    "|3   | 2.183            | 0.5902 ± 0.039 |\n",
    "...\n",
    "\n",
    "CPU times: user 13.4 s, sys: 1.19 s, total: 14.6 s\n",
    "Wall time: 5min 53s\n",
    "\n",
    "We then fixed the value to `2.105`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-estimators\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "adaboosted_pipeline = Pipeline([\n",
    "    ('scaling', ColumnTransformer([\n",
    "        ('pass-through-categorical', 'passthrough', list(range(NB_CATEGORICAL_FEATURES))),\n",
    "        ('scaling-continuous', StandardScaler(copy=False), list(range(NB_CATEGORICAL_FEATURES,NB_FEATURES)))\n",
    "    ])),\n",
    "    (\"dim_reduction\", PCA(n_components=35)),\n",
    "    (CLASSIFIER_PIPELINE_KEY, AdaBoostClassifier(\n",
    "        LinearSVC(\n",
    "            C=2.105,\n",
    "            dual=False,\n",
    "            class_weight=\"balanced\",\n",
    "#             random_state=RANDOM_STATE\n",
    "        ),\n",
    "        algorithm='SAMME',\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "scores = cross_validate(\n",
    "    estimator=adaboosted_pipeline,\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    groups=X_train_valid[:,SUBJECT_IDX],\n",
    "    scoring={\n",
    "        \"agreement\": make_scorer(cohen_kappa_score),\n",
    "        \"accuracy\": 'accuracy',\n",
    "        \"f1-score-macro\": 'f1_macro',\n",
    "        \"f1-score-weighted\": 'f1_weighted',\n",
    "    },\n",
    "    cv=get_cv_iterator(n_splits=2),\n",
    "    return_train_score=True,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print_results_cv_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "```\n",
    "Mean accuracy          : 0.67 ± 0.003\n",
    "Mean macro F1-score    : 0.60 ± 0.005\n",
    "Mean weighted F1-score : 0.66 ± 0.003\n",
    "Mean Kappa's agreement : 0.55 ± 0.001\n",
    "CPU times: user 49.7 ms, sys: 57 ms, total: 107 ms\n",
    "Wall time: 30.5 s\n",
    "```\n",
    "The results have not been improved compared to the same model without the `AdaBoost` meta-estimator.\n",
    "\n",
    "## SVM testing\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "testing_pipeline = get_pipeline(\n",
    "    classifier=LinearSVC(\n",
    "        dual=False,\n",
    "        C=2.105,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    dimension_reduction=PCA(n_components=35)\n",
    ")\n",
    "\n",
    "testing_pipeline.fit(X_train_valid[:, 2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = testing_pipeline.predict(X_test[:,2:])\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results\n",
    "___\n",
    "\n",
    "#### a) Without PCA and without tuning (C=1)\n",
    "___\n",
    "\n",
    "```\n",
    "[[1443   48    7   24  102]\n",
    " [ 244  244  298    4  193]\n",
    " [  71   89 3037  318   88]\n",
    " [   4    0   16  591    0]\n",
    " [  97  161  240    3  801]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.78      0.89      0.83      1624\n",
    "          N1       0.45      0.25      0.32       983\n",
    "          N2       0.84      0.84      0.84      3603\n",
    "          N3       0.63      0.97      0.76       611\n",
    "         REM       0.68      0.62      0.64      1302\n",
    "\n",
    "    accuracy                           0.75      8123\n",
    "   macro avg       0.68      0.71      0.68      8123\n",
    "weighted avg       0.74      0.75      0.74      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.6557078634244826\n",
    "\n",
    "```\n",
    "\n",
    "#### b) Without PCA and with tuning (C=2.105)\n",
    "___\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.78      0.89      0.83      1624\n",
    "          N1       0.45      0.25      0.32       983\n",
    "          N2       0.84      0.84      0.84      3603\n",
    "          N3       0.63      0.97      0.76       611\n",
    "         REM       0.68      0.62      0.64      1302\n",
    "\n",
    "    accuracy                           0.75      8123\n",
    "   macro avg       0.67      0.71      0.68      8123\n",
    "weighted avg       0.74      0.75      0.74      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.6555409806957044\n",
    "```\n",
    "\n",
    "#### c) With PCA and tuning (C=2.105)\n",
    "___\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.77      0.88      0.82      1624\n",
    "          N1       0.44      0.24      0.31       983\n",
    "          N2       0.84      0.84      0.84      3603\n",
    "          N3       0.62      0.97      0.76       611\n",
    "         REM       0.68      0.62      0.64      1302\n",
    "\n",
    "    accuracy                           0.75      8123\n",
    "   macro avg       0.67      0.71      0.68      8123\n",
    "weighted avg       0.74      0.75      0.73      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.6506513702343493\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test subjects are subjects: \", np.unique(X_test[:,0]))\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "\n",
    "for test_subject in np.unique(X_test[:,0]):\n",
    "    test_subject_indexes = [idx for idx, elem in enumerate(X_test) if elem[0] == test_subject]\n",
    "    \n",
    "    for night_idx in np.unique(X_test[test_subject_indexes,1]):\n",
    "        test_night_subject_indexes = [\n",
    "            idx for idx, elem in enumerate(X_test)\n",
    "            if elem[0] == test_subject and elem[1] == night_idx]\n",
    "        hypnograms = [\n",
    "            y_test[test_night_subject_indexes],\n",
    "            y_test_pred[test_night_subject_indexes]\n",
    "        ]\n",
    "        \n",
    "        print_hypnogram(hypnograms,\n",
    "                        labels=[\"scored\", \"predicted\"],\n",
    "                        subject=test_subject,\n",
    "                        night=night_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained model\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR = \"../trained_model\"\n",
    "\n",
    "if not os.path.exists(SAVED_DIR):\n",
    "    os.mkdir(SAVED_DIR);\n",
    "\n",
    "if USE_CONTINUOUS_AGE: \n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_SVC_age_continuous.joblib\")\n",
    "else:\n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_SVC.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
