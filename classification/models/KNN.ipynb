{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour Classification\n",
    "___\n",
    "\n",
    "This model aims to classify sleep stages based on two EEG channel. We will use the features extracted in the `pipeline.ipynb` notebook as the input to a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Ensure parent folder is in PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "                                     RandomizedSearchCV,\n",
    "                                     GroupKFold,\n",
    "                                     cross_validate)\n",
    "from sklearn.metrics import (accuracy_score,\n",
    "                             confusion_matrix,\n",
    "                             classification_report,\n",
    "                             f1_score,\n",
    "                             cohen_kappa_score,\n",
    "                             make_scorer)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from constants import (SLEEP_STAGES_VALUES,\n",
    "                       N_STAGES,\n",
    "                       EPOCH_DURATION)\n",
    "from model_utils import (print_hypnogram,\n",
    "                         train_test_split_one_subject,\n",
    "                         train_test_split_according_to_age,\n",
    "                         evaluate_hyperparams_grid,\n",
    "                         print_results_cv,\n",
    "                         print_results_cv_scores,\n",
    "                         get_pipeline,\n",
    "                         print_hyperparam_tuning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the features\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position of the subject information and night information in the X matrix\n",
    "SUBJECT_IDX = 0 \n",
    "NIGHT_IDX = 1\n",
    "USE_CONTINUOUS_AGE = False\n",
    "DOWNSIZE_SET = False\n",
    "TEST_SET_SUBJECTS = [0.0, 24.0, 49.0, 71.0]\n",
    "\n",
    "if USE_CONTINUOUS_AGE:\n",
    "    X_file_name = \"../data/x_features-age-continuous.npy\"\n",
    "    y_file_name = \"../data/y_observations-age-continuous.npy\"\n",
    "else:\n",
    "    X_file_name = \"../data/x_features.npy\"\n",
    "    y_file_name = \"../data/y_observations.npy\"\n",
    "    \n",
    "X_init = np.load(X_file_name, allow_pickle=True)\n",
    "y_init = np.load(y_file_name, allow_pickle=True)\n",
    "\n",
    "X_init = np.vstack(X_init)\n",
    "y_init = np.hstack(y_init)\n",
    "print(X_init.shape)\n",
    "print(y_init.shape)\n",
    "print(\"Number of subjects: \", np.unique(X_init[:,SUBJECT_IDX]).shape[0]) # Some subject indexes are skipped, thus total number is below 83 (as we can see in https://physionet.org/content/sleep-edfx/1.0.0/)\n",
    "print(\"Number of nights: \", len(np.unique([f\"{int(x[0])}-{int(x[1])}\" for x in X_init[:,SUBJECT_IDX:NIGHT_IDX+1]])))\n",
    "print(\"Subjects available: \", np.unique(X_init[:,SUBJECT_IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_train_valid, y_test, y_train_valid = train_test_split_according_to_age(\n",
    "    X_init,\n",
    "    y_init,\n",
    "    use_continuous_age=USE_CONTINUOUS_AGE,\n",
    "    subjects_test=TEST_SET_SUBJECTS)\n",
    "    \n",
    "print(X_test.shape, X_train_valid.shape, y_test.shape, y_train_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN validation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_KFOLDS = 5\n",
    "NB_CATEGORICAL_FEATURES = 2\n",
    "NB_FEATURES = 48\n",
    "CLASSIFIER_PIPELINE_KEY = 'classifier'\n",
    "RANDOM_STATE = 42 \n",
    "\n",
    "def get_cv_iterator(n_splits=2):\n",
    "    return GroupKFold(n_splits=n_splits).split(\n",
    "        X_train_valid, groups=X_train_valid[:,SUBJECT_IDX]\n",
    "    )\n",
    "    \n",
    "def cross_validate_with_confusion_matrix(pipeline, n_fold):\n",
    "    accuracies = []\n",
    "    macro_f1_scores = []\n",
    "    weighted_f1_scores = []\n",
    "    kappa_agreements = []\n",
    "\n",
    "    for train_index, valid_index in get_cv_iterator(n_splits=n_fold):\n",
    "        # We drop the subject and night indexes\n",
    "        X_train, X_valid = X_train_valid[train_index, 2:], X_train_valid[valid_index, 2:]\n",
    "        y_train, y_valid = y_train_valid[train_index], y_train_valid[valid_index]\n",
    "\n",
    "        # Scaling features and model training\n",
    "        training_pipeline = pipeline\n",
    "        training_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Validation\n",
    "        y_valid_pred = training_pipeline.predict(X_valid)\n",
    "\n",
    "        print(\"----------------------------- FOLD RESULTS --------------------------------------\\n\")\n",
    "        current_kappa = cohen_kappa_score(y_valid, y_valid_pred)\n",
    "\n",
    "        print(\"TRAIN:\", train_index, \"VALID:\", valid_index, \"\\n\\n\")\n",
    "        print(confusion_matrix(y_valid, y_valid_pred), \"\\n\")\n",
    "        print(classification_report(y_valid, y_valid_pred, target_names=SLEEP_STAGES_VALUES.keys()), \"\\n\")\n",
    "        print(\"Agreement score (Cohen Kappa): \", current_kappa, \"\\n\")\n",
    "\n",
    "        accuracies.append(round(accuracy_score(y_valid, y_valid_pred),2))\n",
    "        macro_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"macro\"))\n",
    "        weighted_f1_scores.append(f1_score(y_valid, y_valid_pred, average=\"weighted\"))\n",
    "        kappa_agreements.append(current_kappa)\n",
    "\n",
    "    print_results_cv(accuracies, macro_f1_scores, weighted_f1_scores, kappa_agreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_confusion_matrix(get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        n_jobs=-1\n",
    "    )\n",
    "), n_fold=NB_KFOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are lower than the other models (SVC,RF), and takes much longer to train.\n",
    "\n",
    "```\n",
    "Mean accuracy          : 0.67 ± 0.031\n",
    "Mean macro F1-score    : 0.60 ± 0.029\n",
    "Mean weighted F1-score : 0.66 ± 0.032\n",
    "Mean Kappa's agreement : 0.55 ± 0.044\n",
    "CPU times: user 43min 2s, sys: 26.7 s, total: 43min 29s\n",
    "Wall time: 20min 36s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_clf = get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        n_jobs=-1)\n",
    ")\n",
    "\n",
    "knn_clf.fit(X_train_valid[:150000,2:], y_train_valid[:150000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_clf.predict(X_train_valid[150000:,2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, training time is really fast, while the prediction is quite slow. It can be explained by how the KNN model works: \n",
    "\n",
    "> [...] it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\n",
    "\n",
    "## Validation results\n",
    "___\n",
    "\n",
    "### Dimension reduction\n",
    "___\n",
    "\n",
    "As with the other models, we will use LDA and PCA to reduce dimensions. We will try the maintain the same scores, and reduce the time it takes to cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_with_dim_reduction(dim_reduction):\n",
    "    pipeline = get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=dim_reduction\n",
    "    )\n",
    "    \n",
    "    scores = cross_validate(\n",
    "        estimator=pipeline,\n",
    "        X=X_train_valid,\n",
    "        y=y_train_valid,\n",
    "        groups=X_train_valid[:,SUBJECT_IDX],\n",
    "        scoring={\n",
    "            \"agreement\": make_scorer(cohen_kappa_score),\n",
    "            \"accuracy\": 'accuracy',\n",
    "            \"f1-score-macro\": 'f1_macro',\n",
    "            \"f1-score-weighted\": 'f1_weighted',\n",
    "        },\n",
    "        cv=get_cv_iterator(n_splits=5),\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print_results_cv_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. LDA\n",
    "___\n",
    "\n",
    "We have `n_components=4` by default when using LDA to reduce dimensionality. We can see it speeds up a lot (15.8s with vs ~20min without) the prediction time for the same number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. PCA\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cross_validate_with_dim_reduction(PCA(n_components=35))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "___\n",
    "\n",
    "|  Score             |  without      | LDA           | PCA (n_comp=4) | PCA (n_comp=16) | PCA (n_comp=35) |\n",
    "|--------------------|---------------|---------------|----------------|-----------------|-----------------|\n",
    "|  accuracy          | 0.67 ± 0.031  |  0.66 ± 0.031 | 0.50 ± 0.022   |  0.63 ± 0.037   |  0.63 ± 0.036   |\n",
    "|  macro F1-score    | 0.60 ± 0.029  |  0.59 ± 0.024 | 0.45 ± 0.009   |  0.57 ± 0.033   |  0.57 ± 0.032   |\n",
    "|  weighted F1-score | 0.66 ± 0.032  |  0.65 ± 0.028 | 0.50 ± 0.020   |  0.63 ± 0.035   |  0.63 ± 0.034   |\n",
    "|  Kappa's agreement | 0.55 ± 0.044  |  0.53 ± 0.040 | 0.32 ± 0.025   |  0.49 ± 0.050   |  0.50 ± 0.048   |\n",
    "|  Time              | 20min 36s     |  15.8 s       | 15.3 s         |  52.4 s         |  1min 42s       |\n",
    "\n",
    "The results with LDA used as dimension reduction have slightly worst results thant without, but has the best overall score accross PCA and LDA scores. We will keep LDA as a step in our pipeline, because it speeds up a lot prediction time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "___\n",
    "\n",
    "The hyperparameters of a KNN classifier are:\n",
    "- `n_neighbors`: `int` (`default=5`)\n",
    "    \n",
    "    Represents the number of neighbors which votes the predicted class\n",
    "- `weights`: `{‘uniform’, ‘distance’} or callable, default='uniform'`\n",
    "\n",
    "    Weight functions attributed to neighbors in the voting process. If `uniform`, all neighbors have the same weight, whilst setting `weights` to `distance` involves that closer neighbors of a query point will have a greater influence than neighbors which are further away.\n",
    "\n",
    "- `leaf_size`: `positive int` (`default=30`)\n",
    "\n",
    "    Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree. **The optimal value depends on the nature of the problem.**\n",
    "    \n",
    "- `metric`: `str or callable, default=’minkowski’`\n",
    "\n",
    "    The distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. \n",
    "    With hyperparam `p`, default value is euclidean distance. We could also look at `manhattan` and `chebyshev`.\n",
    "    \n",
    "### 1. Hyperparameters `n_neighbors` and `weights`\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__n_neighbors\": np.linspace(200, 500, 10, dtype=\"int\"),\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__weights\": ['uniform'],\n",
    "\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=2),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st experiment**: `n_neighbors\": [  2,  57, 112, 168, 223, 278, 334, 389, 444, 500]` with `weights=['uniform', 'distance']`\n",
    "\n",
    "|Rank| n_neighbors      |  weights          | Test score     |\n",
    "|----|------------------|-------------------|----------------|\n",
    "|1   | 389              | 'uniform'         | 0.5815 ± 0.001 |\n",
    "|2   | 223              | 'uniform'         | 0.5814 ± 0.002 |\n",
    "|3   | 334              | 'uniform'         | 0.5813 ± 0.002 |\n",
    "|4   | 500              | 'uniform'         | 0.5812 ± 0.002 |\n",
    "| ... |\n",
    "|13   | 57               | 'distance'        | 0.5766 ± 0.004 |\n",
    "|14   | 2                | 'distance'        | 0.4655 ± 0.001 |\n",
    "|15   | 2                | 'uniform'         | 0.4589 ± 0.014 |\n",
    "\n",
    "1st run:\n",
    "\n",
    "1. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 389} has a score of 0.5815 ± 0.001\n",
    "2. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 223} has a score of 0.5814 ± 0.002\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 500} has a score of 0.5812 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 500} has a score of 0.5810 ± 0.002\n",
    "5. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 223} has a score of 0.5809 ± 0.003\n",
    "6. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 168} has a score of 0.5806 ± 0.003\n",
    "7. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 112} has a score of 0.5797 ± 0.004\n",
    "8. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 57} has a score of 0.5774 ± 0.005\n",
    "9. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 2} has a score of 0.4655 ± 0.001\n",
    "10. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 2} has a score of 0.4589 ± 0.014\n",
    "\n",
    "2nd run (without duplicates):\n",
    "\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 334} has a score of 0.5813 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 168} has a score of 0.5810 ± 0.003\n",
    "5. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 444} has a score of 0.5809 ± 0.001\n",
    "6. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 112} has a score of 0.5807 ± 0.004\n",
    "9. Parameter {'classifier__weights': 'distance', 'classifier__n_neighbors': 57} has a score of 0.5766 ± 0.004\n",
    "\n",
    "We can see that the best results comes with uniform weights and `n_neighbors` in the range [200,500].\n",
    "\n",
    "___\n",
    "\n",
    "**2nd experiment**: `n_neighbors\": [200, 233, 266, 300, 333, 366, 400, 433, 466, 500]` with `weights=['uniform']`\n",
    "\n",
    "|Rank| n_neighbors      |  weights          | Test score     |\n",
    "|----|------------------|-------------------|----------------|\n",
    "|1   | 200              | 'uniform'         | 0.5817 ± 0.003 |\n",
    "|2   | 300              | 'uniform'         | 0.5817 ± 0.001 |\n",
    "|3   | 233              | 'uniform'         | 0.5816 ± 0.002 |\n",
    "|4   | 400              | 'uniform'         | 0.5815 ± 0.002 |\n",
    "| ... |\n",
    "|9    | 266              | 'uniform'         | 0.5812 ± 0.002 |\n",
    "|10   | 366              | 'uniform'         | 0.5812 ± 0.001 |\n",
    "\n",
    "1. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 200} has a score of 0.5817 ± 0.003\n",
    "2. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 300} has a score of 0.5817 ± 0.001\n",
    "3. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 233} has a score of 0.5816 ± 0.002\n",
    "4. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 400} has a score of 0.5815 ± 0.002\n",
    "5. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 466} has a score of 0.5814 ± 0.002\n",
    "6. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 433} has a score of 0.5813 ± 0.001\n",
    "7. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 500} has a score of 0.5812 ± 0.002\n",
    "8. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 333} has a score of 0.5812 ± 0.002\n",
    "9. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 266} has a score of 0.5812 ± 0.002\n",
    "10. Parameter {'classifier__weights': 'uniform', 'classifier__n_neighbors': 366} has a score of 0.5812 ± 0.001\n",
    "CPU times: user 3.19 s, sys: 737 ms, total: 3.92 s\n",
    "Wall time: 3min 30s\n",
    "\n",
    "**We will keep `weights: 'uniform'`, `n_neighbors': 200`.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperparameter `leaf_size`\n",
    "____\n",
    "\n",
    "We will try to find the value that minimizes prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    param_grid={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__leaf_size\": np.linspace(10, 100, 10, dtype=\"int\")\n",
    "    },\n",
    "    scoring=make_scorer(cohen_kappa_score),\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train_valid[:,2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, current_param in enumerate(search.cv_results_['params']):\n",
    "    mean_fit_time = search.cv_results_['mean_fit_time'][idx]\n",
    "    std_fit_time = search.cv_results_['std_fit_time'][idx]\n",
    "    mean_score_time = search.cv_results_['mean_score_time'][idx]\n",
    "    std_score_time = search.cv_results_['std_score_time'][idx]\n",
    "    print(f\"Parameter {current_param} has a score time of {mean_score_time:0.4f}s ± {std_score_time:0.3f}s and fit time of {mean_fit_time:0.4f} ± {std_fit_time:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st experiment**: `leaf_size=[10,20,30,...,90,100]`\n",
    "\n",
    "|Rank| leaf_size     |  Score time       | Fit time       |\n",
    "|----|---------------|-------------------|----------------|\n",
    "|1   | 100           | 3.0002s ± 1.039s  | 4.0782 ± 0.958 |\n",
    "|2   | 60            | 3.1930s ± 0.377s  | 4.8671 ± 0.521 |\n",
    "|3   | 40            | 3.9384s ± 0.762s  | 6.5524 ± 0.852 |\n",
    "|4   | 80            | 3.9545s ± 0.686s  | 5.0560 ± 0.466 |\n",
    "| ... |\n",
    "|9   | 30            | 4.4972s ± 0.774s  | 7.0859 ± 0.918 |\n",
    "|10  | 20            | 5.0833s ± 0.358s  | 6.9917 ± 0.407 |\n",
    "\n",
    "We will choose `leaf_size=100`.\n",
    "\n",
    "### 3. `metric` hyperparameter\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__metric\": ['minkowski', 'manhattan', 'chebyshev']\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rank| metric        |  Score          |\n",
    "|----|---------------|-----------------|\n",
    "|1   | 'manhattan'   | 0.5348 ± 0.038  |\n",
    "|2   | 'chebyshev'   | 0.5336 ± 0.038  |\n",
    "|3   | 'minkowski'   | 0.5333 ± 0.039  |\n",
    "\n",
    "We will choose `metric='manhattan'`.\n",
    "\n",
    "### Checking final hyperparameters\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_hyperparams_grid(\n",
    "    params={\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__metric\": ['manhattan', 'chebyshev'],\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__n_neighbors\": np.linspace(200, 300, 3, dtype=\"int\"),\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__weights\": ['uniform'],\n",
    "        f\"{CLASSIFIER_PIPELINE_KEY}__leaf_size\": [200]\n",
    "\n",
    "    },\n",
    "    estimator=get_pipeline(\n",
    "        classifier=KNeighborsClassifier(n_jobs=-1),\n",
    "        dimension_reduction=LinearDiscriminantAnalysis()\n",
    "    ),\n",
    "    X=X_train_valid,\n",
    "    y=y_train_valid,\n",
    "    cv=get_cv_iterator(n_splits=5),\n",
    "    use_randomized=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Rank| n_neighbors   |  metric           | Score          |\n",
    "|----|---------------|-------------------|----------------|\n",
    "|1   | 300           | chebyshev         | 0.5820 ± 0.044 |\n",
    "|2   | 250           | manhattan         | 0.5819 ± 0.044 |\n",
    "|3   | 250           | chebyshev         | 0.5817 ± 0.044 |\n",
    "|4   | 200           | manhattan         | 0.5815 ± 0.044 |\n",
    "|5   | 300           | manhattan         | 0.5814 ± 0.044 |\n",
    "|6   | 200           | chebyshev         | 0.5813 ± 0.044 |\n",
    "\n",
    "We had previously set the hyperparamters the following value, by independantly running tests:\n",
    "```\n",
    "weights='uniform',\n",
    "n_neighbors=200,\n",
    "leaf_size=100,\n",
    "metric='manhattan',\n",
    "```\n",
    "\n",
    "By checking all these together, with runner ups for hyperparameters `n_neighbors` and `metric`, we found that the following hyperparameters should be better:\n",
    "```\n",
    "weights='uniform',\n",
    "n_neighbors=300,\n",
    "leaf_size=100,\n",
    "metric='chebyshev',\n",
    "```\n",
    "\n",
    "## Testing \n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "testing_pipeline = get_pipeline(\n",
    "    classifier=KNeighborsClassifier(\n",
    "        weights='uniform',\n",
    "        n_neighbors=300,\n",
    "        leaf_size=100,\n",
    "        metric='chebyshev',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    dimension_reduction=LinearDiscriminantAnalysis()\n",
    ")\n",
    "\n",
    "testing_pipeline.fit(X_train_valid[:, 2:], y_train_valid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = testing_pipeline.predict(X_test[:,2:])\n",
    "\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=SLEEP_STAGES_VALUES.keys()))\n",
    "\n",
    "print(\"Agreement score (Cohen Kappa): \", cohen_kappa_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test results\n",
    "___\n",
    "\n",
    "#### a) With LDA and tuning (metric=manhattan and n_neighbors=200)\n",
    "___\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.75      0.92      0.83      1624\n",
    "          N1       0.44      0.15      0.23       983\n",
    "          N2       0.83      0.87      0.85      3603\n",
    "          N3       0.70      0.92      0.80       611\n",
    "         REM       0.68      0.61      0.64      1302\n",
    "\n",
    "    accuracy                           0.76      8123\n",
    "   macro avg       0.68      0.70      0.67      8123\n",
    "weighted avg       0.73      0.76      0.73      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.656774449223104\n",
    "```\n",
    "\n",
    "#### b) With LDA and tuning (metric=chebyshev and n_neighbors=300)\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           W       0.75      0.92      0.82      1624\n",
    "          N1       0.43      0.15      0.22       983\n",
    "          N2       0.83      0.88      0.85      3603\n",
    "          N3       0.71      0.92      0.80       611\n",
    "         REM       0.68      0.61      0.64      1302\n",
    "\n",
    "    accuracy                           0.76      8123\n",
    "   macro avg       0.68      0.69      0.67      8123\n",
    "weighted avg       0.73      0.76      0.73      8123\n",
    "\n",
    "Agreement score (Cohen Kappa):  0.6566457163531443\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving trained model\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR = \"../trained_model\"\n",
    "\n",
    "if not os.path.exists(SAVED_DIR):\n",
    "    os.mkdir(SAVED_DIR);\n",
    "\n",
    "if USE_CONTINUOUS_AGE: \n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_KNN_age_continuous.joblib\")\n",
    "else:\n",
    "    joblib.dump(testing_pipeline, f\"{SAVED_DIR}/classifier_KNN.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('py3': conda)",
   "language": "python",
   "name": "python36864bitpy3conda60d5576ae6834cf0a5dc3773043eb4d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
